{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82923dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevent libraries \n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from psycopg2 import Error\n",
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "#load_dotenv(\"../.env\")\n",
    "\n",
    "PG_HOST = os.getenv('POSTGRES_HOST')\n",
    "PG_PORT = os.getenv('POSTGRES_PORT')\n",
    "PG_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "PG_USER = os.getenv('POSTGRES_USER')\n",
    "PG_DB = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Read in the data file that we need to insert\n",
    "df = pd.read_excel(\"./Data/Cleaned_Root_letters.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8438139e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_id\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_key\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tup \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m TexttoWord:\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mTexttoWord\u001b[49m\u001b[38;5;241m.\u001b[39mappend( tup )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Clean the data right at the start \n",
    "df = df.convert_dtypes()\n",
    "df['ARABIC'] = df['ARABIC'].str.strip()\n",
    "df['Root_Letters'] = df['Root_Letters'].str.strip()\n",
    "\n",
    "# Prep Data for wRootWord Table s\n",
    "lst = list(df['Root_Letters'])\n",
    "lst = set(lst)\n",
    "\n",
    "# Make it into a dataframe, and give it an index / ID column \n",
    "df_roots = pd.DataFrame(lst, columns=['Root_Words'])\n",
    "df_roots = df_roots.reset_index()\n",
    "\n",
    "# Turn it into string and get rid of white spaces \n",
    "df_roots = df_roots.convert_dtypes()\n",
    "df_roots['Root_Words'] = df_roots['Root_Words'].str.strip()\n",
    "\n",
    "# Put the data in the format the db accepts\n",
    "root_data = []\n",
    "for row in df_roots.index:\n",
    "    arabic_text = df_roots.loc[row,\"Root_Words\"]\n",
    "    unique_id = row\n",
    "    root_data.append((row, arabic_text))\n",
    "\n",
    "# Prep Data for the ArabicWord Table\n",
    "arabic_df = df.drop(['ID', 'Transliteration'], axis=1)\n",
    "arabic_df = arabic_df.drop_duplicates()\n",
    "arabic_df[\"primary_key\"] = list(range(2000,19623)) \n",
    "arabic_df = arabic_df.convert_dtypes()\n",
    "\n",
    "# Merge the df to get the RootWords PK (called \"index\")\n",
    "arabic_words_df = arabic_df.merge(df_roots, how='inner', left_on='Root_Letters', right_on='Root_Words')\n",
    "arabic_words_df = arabic_words_df.drop(['Root_Letters', 'Root_Words'], axis=1)\n",
    "\n",
    "# Rename the index column to RootID to make it clear\n",
    "arabic_words_df.rename(columns = {'index':'RootID'}, inplace = True)\n",
    "\n",
    "# Put the dataframe into the format for postgres sql insertion \n",
    "arabic_words_data = []\n",
    "for index, row in arabic_words_df.iterrows():\n",
    "    arabic_words_data.append( (row['primary_key'], row['ARABIC'], row['RootID']) )\n",
    "    \n",
    "########################################\n",
    "### GET data for  TEXT TO WORD table ###\n",
    "########################################\n",
    "\n",
    "# Create surah and ayah columns \n",
    "surah, ayah = [], []\n",
    "for row in df[\"ID\"]:\n",
    "    surah.append( row.split(\":\")[0] ) \n",
    "    ayah.append( row.split(\":\")[1] ) \n",
    "df[\"Surah\"] = surah\n",
    "df[\"Ayah\"] = ayah\n",
    "df[\"Surah\"] = df[\"Surah\"].values.astype(int)\n",
    "df[\"Ayah\"] = df[\"Ayah\"].values.astype(int)\n",
    "\n",
    "# Get the IndexIDs from the Quran Table  -- need to merge it since different lengths\n",
    "indexID, count = [], 0\n",
    "ayahs = df['Ayah'].to_list()\n",
    "# Essentially generating each ayah = 1 unique id, from 1 - 6000. # Don't need quran table now\n",
    "for i in range(len(ayahs)):\n",
    "    if ayahs[i] != ayahs[i-1]:\n",
    "        count+=1 \n",
    "        indexID.append(count)\n",
    "    else:\n",
    "        indexID.append(count)\n",
    "    \n",
    "df['index_id'] = indexID\n",
    "\n",
    "# Generate our text to words table\n",
    "TextToWords_df = df.merge(arabic_words_df, left_on =\"ARABIC\", right_on = \"ARABIC\", how = 'inner')\n",
    "TextToWords_df = TextToWords_df.reset_index()\n",
    "\n",
    "# Put the dataframe into the format for postgres sql insertion \n",
    "TexttoWord = []\n",
    "for index, row in TextToWords_df.iterrows():\n",
    "    tup = tuple((row['index_id'], row['primary_key']))\n",
    "    if tup not in TexttoWord:\n",
    "        TexttoWord.append( tup )\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "\n",
    "    # connect to the PostgreSQL database\n",
    "    connection = psycopg2.connect(user=PG_USER,\n",
    "                              password=PG_PASSWORD,\n",
    "                              host=PG_HOST,\n",
    "                              port=PG_PORT,\n",
    "                              database=PG_DB)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)\n",
    "    \n",
    "# Create functions to insert root words, arabic words and text to word data   \n",
    "def insert_root_words(root_words_data):\n",
    "    \"\"\" Insert Root Words Data into the POSTGRES DB \"\"\"\n",
    "    postgres_insert_query = \"\"\" INSERT INTO RootWord (RootID, RootWord) VALUES (%s ,%s)\"\"\"\n",
    "    counter = 0\n",
    "    for row in root_words_data:\n",
    "        record_to_insert = row\n",
    "        cursor.execute(postgres_insert_query, record_to_insert)\n",
    "        connection.commit()\n",
    "        counter += cursor.rowcount\n",
    "    print(counter, \"records inserted successfully into rootWord table\")\n",
    "    return counter\n",
    "\n",
    "def insert_arabic_text(arabic_text_data):\n",
    "    \"\"\" Insert Arabic Text Data into the POSTGRES DB \"\"\"\n",
    "    postgres_insert_query = \"\"\" INSERT INTO ArabicWord (WordID, Word, RootID) VALUES (%s ,%s, %s)\"\"\"\n",
    "    counter = 0\n",
    "    for row in arabic_text_data:\n",
    "        record_to_insert = row\n",
    "        cursor.execute(postgres_insert_query, record_to_insert)\n",
    "        connection.commit()\n",
    "        counter += cursor.rowcount\n",
    "    print(counter, \"records inserted successfully into ArabicWord table\")\n",
    "    return counter\n",
    "\n",
    "# Create function to get the quran text data \n",
    "def get_quran_text_df():\n",
    "    \"\"\" Get the quran text dataframe \"\"\"\n",
    "    postgreSQL_select_Query = \"select * from quran_text\"\n",
    "    cursor.execute(postgreSQL_select_Query)\n",
    "    print(\"Selecting rows from table using cursor.fetchall\")\n",
    "    sql_table_result = cursor.fetchall()\n",
    "    quran_text_df = pd.DataFrame(sql_table_result, columns=['index', 'sura', 'aya','text'])\n",
    "    return quran_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe131dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7874d5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a73a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script below. Don't touch unless you want to run the whole thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    # connect to the PostgreSQL database\n",
    "    connection = psycopg2.connect(user=PG_USER,\n",
    "                              password=PG_PASSWORD,\n",
    "                              host=PG_HOST,\n",
    "                              port=PG_PORT,\n",
    "                              database=PG_DB)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b27e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevent libraries \n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from psycopg2 import Error\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "PG_HOST = os.getenv('POSTGRES_HOST')\n",
    "PG_PORT = os.getenv('POSTGRES_PORT')\n",
    "PG_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "PG_USER = os.getenv('POSTGRES_USER')\n",
    "PG_DB = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Read in the data file that we need to insert\n",
    "df = pd.read_excel(\"./Data/Cleaned_Root_letters.xlsx\")\n",
    "\n",
    "# Clean the data right at the start \n",
    "df = df.convert_dtypes()\n",
    "df['ARABIC'] = df['ARABIC'].str.strip()\n",
    "df['Root_Letters'] = df['Root_Letters'].str.strip()\n",
    "\n",
    "# Prep Data for RootWord Table \n",
    "lst = list(df['Root_Letters'])\n",
    "lst = set(lst)\n",
    "\n",
    "# Make it into a dataframe, and give it an index / ID column \n",
    "df_roots = pd.DataFrame(lst, columns=['Root_Words'])\n",
    "df_roots = df_roots.reset_index()\n",
    "\n",
    "# Turn it into string and get rid of white spaces \n",
    "df_roots = df_roots.convert_dtypes()\n",
    "df_roots['Root_Words'] = df_roots['Root_Words'].str.strip()\n",
    "\n",
    "# Put the data in the format the db accepts\n",
    "root_data = []\n",
    "for row in df_roots.index:\n",
    "    arabic_text = df_roots.loc[row,\"Root_Words\"]\n",
    "    unique_id = row\n",
    "    root_data.append((row, arabic_text))\n",
    "\n",
    "# Prep Data for the ArabicWord Table\n",
    "arabic_df = df.drop(['ID', 'Transliteration'], axis=1)\n",
    "arabic_df = arabic_df.drop_duplicates()\n",
    "arabic_df[\"primary_key\"] = list(range(2000,19623)) \n",
    "arabic_df = arabic_df.convert_dtypes()\n",
    "\n",
    "# Merge the df to get the RootWords PK (called \"index\")\n",
    "arabic_words_df = arabic_df.merge(df_roots, how='inner', left_on='Root_Letters', right_on='Root_Words')\n",
    "arabic_words_df = arabic_words_df.drop(['Root_Letters', 'Root_Words'], axis=1)\n",
    "\n",
    "# Rename the index column to RootID to make it clear\n",
    "arabic_words_df.rename(columns = {'index':'RootID'}, inplace = True)\n",
    "\n",
    "# Put the dataframe into the format for postgres sql insertion \n",
    "arabic_words_data = []\n",
    "for index, row in arabic_words_df.iterrows():\n",
    "    arabic_words_data.append( (row['primary_key'], row['ARABIC'], row['RootID']) )\n",
    "    \n",
    "########################################\n",
    "### GET data for  TEXT TO WORD table ###\n",
    "########################################\n",
    "\n",
    "# Create surah and ayah columns \n",
    "surah, ayah = [], []\n",
    "for row in df[\"ID\"]:\n",
    "    surah.append( row.split(\":\")[0] ) \n",
    "    ayah.append( row.split(\":\")[1] ) \n",
    "df[\"Surah\"] = surah\n",
    "df[\"Ayah\"] = ayah\n",
    "df[\"Surah\"] = df[\"Surah\"].values.astype(int)\n",
    "df[\"Ayah\"] = df[\"Ayah\"].values.astype(int)\n",
    "\n",
    "# Get the IndexIDs from the Quran Table  -- need to merge it since different lengths\n",
    "indexID, count = [], 0\n",
    "ayahs = df['Ayah'].to_list()\n",
    "# Essentially generating each ayah = 1 unique id, from 1 - 6000. # Don't need quran table now\n",
    "for i in range(len(ayahs)):\n",
    "    if ayahs[i] != ayahs[i-1]:\n",
    "        count+=1 \n",
    "        indexID.append(count)\n",
    "    else:\n",
    "        indexID.append(count)\n",
    "    \n",
    "df['index_id'] = indexID\n",
    "\n",
    "# Generate our text to words table\n",
    "TextToWords_df = df.merge(arabic_words_df, left_on =\"ARABIC\", right_on = \"ARABIC\", how = 'inner')\n",
    "TextToWords_df = TextToWords_df.reset_index()\n",
    "\n",
    "# Put the dataframe into the format for postgres sql insertion \n",
    "TexttoWord = []\n",
    "for index, row in TextToWords_df.iterrows():\n",
    "    tup = tuple((row['index_id'], row['primary_key']))\n",
    "    if tup not in TexttoWord:\n",
    "        TexttoWord.append( tup )\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "\n",
    "    # connect to the PostgreSQL database\n",
    "    connection = psycopg2.connect(user=PG_USER,\n",
    "                              password=PG_PASSWORD,\n",
    "                              host=PG_HOST,\n",
    "                              port=PG_PORT,\n",
    "                              database=PG_DB)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)\n",
    "    \n",
    "# Create functions to insert root words, arabic words and text to word data   \n",
    "def insert_root_words(root_words_data):\n",
    "    \"\"\" Insert Root Words Data into the POSTGRES DB \"\"\"\n",
    "    postgres_insert_query = \"\"\" INSERT INTO RootWord (RootID, RootWord) VALUES (%s ,%s)\"\"\"\n",
    "    counter = 0\n",
    "    for row in root_words_data:\n",
    "        record_to_insert = row\n",
    "        cursor.execute(postgres_insert_query, record_to_insert)\n",
    "        connection.commit()\n",
    "        counter += cursor.rowcount\n",
    "    print(counter, \"records inserted successfully into rootWord table\")\n",
    "    return counter\n",
    "\n",
    "def insert_arabic_text(arabic_text_data):\n",
    "    \"\"\" Insert Arabic Text Data into the POSTGRES DB \"\"\"\n",
    "    postgres_insert_query = \"\"\" INSERT INTO ArabicWord (WordID, Word, RootID) VALUES (%s ,%s, %s)\"\"\"\n",
    "    counter = 0\n",
    "    for row in arabic_text_data:\n",
    "        record_to_insert = row\n",
    "        cursor.execute(postgres_insert_query, record_to_insert)\n",
    "        connection.commit()\n",
    "        counter += cursor.rowcount\n",
    "    print(counter, \"records inserted successfully into ArabicWord table\")\n",
    "    return counter\n",
    "\n",
    "# Create function to get the quran text data \n",
    "def get_quran_text_df():\n",
    "    \"\"\" Get the quran text dataframe \"\"\"\n",
    "    postgreSQL_select_Query = \"select * from quran_text\"\n",
    "    cursor.execute(postgreSQL_select_Query)\n",
    "    print(\"Selecting rows from table using cursor.fetchall\")\n",
    "    sql_table_result = cursor.fetchall()\n",
    "    quran_text_df = pd.DataFrame(sql_table_result, columns=['index', 'sura', 'aya','text'])\n",
    "    return quran_text_df\n",
    "# Insert the root_words data \n",
    "insert_root_words(root_data)\n",
    "\n",
    "# Insert the arabic words data \n",
    "insert_arabic_text(arabic_words_data)\n",
    "\n",
    "def insert_text_to_word(text_to_word_data):\n",
    "    \"\"\" Insert Text to Word  Data into the POSTGRES DB \"\"\"\n",
    "    postgres_insert_query = \"\"\" INSERT INTO TextToWord (IndexID, WordID) VALUES (%s, %s)\"\"\"\n",
    "    counter = 0\n",
    "    for row in text_to_word_data:\n",
    "        record_to_insert = row\n",
    "        cursor.execute(postgres_insert_query, record_to_insert)\n",
    "        connection.commit()\n",
    "        counter += cursor.rowcount\n",
    "    print(counter, \"records inserted successfully into TextToWord table\")\n",
    "    return counter\n",
    "\n",
    "# Insert text_to_word_df\n",
    "insert_text_to_word(TexttoWord)\n",
    "\n",
    "# Close the DB connection \n",
    "cursor.close()\n",
    "connection.close()\n",
    "print(\"PostgreSQL connection is closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
